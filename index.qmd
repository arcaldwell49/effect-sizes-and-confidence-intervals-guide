---
title: "Effect Sizes and Confidence Intervals Guide"
blank-lines-above-title: 2
shorttitle: "Effect Size Guide"
author:
  - name: Matthew B. Jané
    email: matthew.jane@uconn.edu
    url: https://matthewbjane.com
    orcid: 0000-0002-3121-7769
    affiliations:
      - id: id1
        name: University of Connecticut
        department: Department of Psychological Sciences
  - name: Qinyu Xiao
    email: qinyu.xiao@univie.ac.at
    orcid: 0000-0002-9824-9247
    affiliations:
      - id: id2
        name: University of Vienna
        department: Department of Occupational, Economic, and Social Psychology
  - name: Siu Kit Yeung
    email: yskjdmmh@gmail.com
    orcid: 0000-0002-5835-0981
    affiliations:
      - id: id3
        name: University of Hong Kong
        department: Department of Psychology
  - name: Daniel J. Dunleavy
    email: djd09e@my.fsu.edu
    orcid: 0000-0002-3597-7714
    affiliations:
      - id: id4
        name: Florida State
        department: College of Social Work
  - name: Lukas Röseler
    email: roeseler.lukas@gmail.com
    orcid: 0000-0002-6446-1901
    affiliations:
      - id: id5
        name: University of Bamberg
  - name: Mahmoud Elsherif
    email: mahmoud.medhat.elsherif@gmail.com
    orcid: 0000-0002-0540-3998
    affiliations:
      - id: id6
        name: University of Birmingham
  - name: Denis Cousineau
    email: denis.cousineau@uottawa.ca
    orcid: 0000-0001-5908-0402
    affiliations:
      - id: id7
        name: Université d’Ottawa
  - name: Gilad Feldman
    corresponding: true
    email: gfeldman@hku.hk
    orcid: 0000-0003-2812-6599
    affiliations:
      - id: id8
        name: Department of Psychology
        department: University of Hong Kong
author-note:
  blank-lines-above-author-note: 1
  disclosures:
    study-registration: ~
    data-sharing: ~ 
    conflict-of-interest: "The author(s) declared no potential conflicts of interests with respect to the authorship and/or publication of this article."
    financial-support: "The author(s) received no financial support for the research and/or authorship of this article."
    gratitude: "Thank you to Bo Ley Cheng, Katy Tam, and Kristy for their contributions to this project"
    authorship-agreements: ~
abstract: "This effect sizes and confidence intervals collaborative guide aims to provide students and early-career researchers with hands-on, step-by-step instructions for calculating effect sizes and confidence intervals for common statistical tests used in psychology, social sciences and behavioral sciences, particularly when original data are not available and when reported information is incomplete. It also introduces general background information on effect sizes and confidence intervals, as well as useful R packages for their calculation. Many of the methods and procedures described in this Guide are based on R or R-based Shiny Apps developed by the science community. We were motivated to focus on R as we aim to maximize the reproducibility of our research outcomes and encourage the most reproducible study planning and data analysis workflow, though we also document other methods whenever possible for the reference of our readers. We regularly update this open educational resource, as packages are updated frequently and new packages are developed from time to time in this rapidly changing Open Scholarship era."
keywords: [effect size, confidence interval, collaboration, open science, open educational resource]
# I like using Zotero with BetterBibTeX to output a continuously updated "Better CSL JSON" file. But BibTeX works, too.
bibliography: bibliography.bib
format:
  apaquarto-docx: default
  html:
    theme: zephyr
    toc: true
    toc-location: left
    toc-title: Effect Size Guide
    # apaquarto-pdf:
    #   header-includes:
    #     - \usepackage{longtable}
    #   documentmode: man
    # # can be 10pt, 11pt, or 12pt
    # fontsize: 12pt
    # # Integrate tables and figures in text
    # floatsintext: false
    # # a4 paper if true, letter size if false
    # a4paper: false
    # # suppresses loading of the lmodern font package
    # nolmodern: false
    # # Suppresses loading of the fontenc package
    # nofontenc: false
    # # Suppress the title above the introduction
    # donotrepeattitle: false
    # # In jou mode, use times or pslatex instead of txfonts
    # notxfonts: false
    # # In jou mode, use Computer Modern font instead of times
    # notimes: false
    # # In jou mode, cancels automatic stretching of tables to column width 
    # notab: false
    # # Uses Helvetica font in stu and man mode
    # helv: false
    # # In man and stu mode, neutralizes the \helvetica command
    # nosf: false
    # # In man and stu mode, uses typewriter font
    # tt: false
    # # Puts draft watermark on first page
    # draftfirst: false
    # # Puts draft watermark on all pages
    # draftall: false
    # # Masks references that are marked as the author's own
    # mask: false
    # journal: ~
    # volume: ~
    # course: ~
    # professor: ~
    # duedate: ~
    # # Hides correspondence text
    # nocorrespondence: false
---

::: {.content-hidden when-format="html"}
{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}
:::

*Note.* This is a constantly updated collaborative guide on effect sizes and confidence intervals. The most up-to-date version of this guide is hosted as a Google Doc at this link: <https://mgto.org/effectsizeguide>. A similar guide on power analysis can be found at: <https://mgto.org/poweranalysisguide>. This guide is shared under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.

```{r,message=FALSE,echo=FALSE}
#| label: setup
library(conflicted)
library(tidyverse)
library(flextable)
library(ftExtra)
library(officer)
library(knitr)
library(ggplot2)
conflicts_prefer(dplyr::filter, .quiet = TRUE)
conflicts_prefer(flextable::separate_header, .quiet = TRUE)
```

## Guidelines for contribution

All are encouraged to contribute to this Guide. Please note that this Guide is in continuous development such that it will remain a work in progress for an indefinite period of time. This is intended because we hope the Guide to always reflect the state of the art on the topics of effect sizes and confidence intervals.

### Notes

-   Please use the headings and style as set forth in this document. You can use keyboard shortcuts such as Ctrl + Alt + 1/2/3. The normal text is in Times New Roman font, font size 11. The codes are formatted using the Code Blocks add-on of Google Docs, github theme, font size 8.

-   Use the Suggesting mode rather than the Editing mode. Suggesting is now the default mode for this document. Therefore, please do not hesitate to correct mistakes or modify the contents directly.

-   Add a comment to the document if you find anything missing or improper, or if you feel that things are better organized in a different way. We appreciate your suggestions. If you have any questions, please also add a comment. We will reply and seek to clarify in the document body.

-   Please make proper citations (in APA 7th format) and provide relevant links when you refer to any source that is not your own.

### Credit and authorship

If you believe you have made sufficient contribution that qualifies you as an author, and you would like to be listed as an author of this Guide, please do not hesitate and list your name and contact information below. The administrators (Q.-Y. X., S. K. Y., and G. F.) of this Guide will verify your contribution and add you to the author list. We welcome comments from any person, regardless of whether they want to be an author. You are also welcome to request content to be added to this Guide (please see the Things to add to the guide section in the end).

The authorship order is such that Q.-Y. X. and S. K. Y. will be the first two authors and G. F. will be the last and the corresponding author. All other contributors will be listed alphabetically in the middle and are all considered joint third authors. Contributors are by default given investigation, writing - original draft, and writing - review & editing CRediT authorship roles. It is possible to take on more roles if contributors prefer. Any change in this authorship order rule will have to be approved by all who are already listed as an author.

## Evaluating and Interpreting Confidence Intervals

Effect sizes quantify the magnitude of effects (i.e., strength of a relationship, size of a difference), which are the outcomes of our empirical research. Effect sizes are by no means a new concept. However, reporting them remained largely optional for many years, and only until recently does it become a community standard: scientists now see reporting effect sizes (in addition to the traditional statistical significance) as a must and journals also start to require such reporting. Notably, in 2001 and 2010, The Publication Manual of the American Psychological Association 5th and 6th editions emphasized that it is "almost always necessary"[^1] to report effect sizes [@association2010, pp. 34; see @fritz2012, which provides a comprehensive summary on history and importance of effect size reporting].

[^1]: The qualification ("almost always") was that "multiple-degree-of-freedom effect indicators tend to be less useful than effect indicators that decompose multiple degree-of-freedom tests into meaningful one degree-of-freedom effects" (p. 26). "One degree-of-freedom effects" refer to those associated with contrasts, t-tests, F-tests with numerator $df = 1$, and $1-df$ Chi-square tests, whereas "multiple-degree-of-freedom effects" refer to those associated with, for instance, F-tests with numerator $df > 1$, and Chi-square tests with $df > 1$.

Effects sizes can be grouped in broad categories as (1) raw effect sizes, and (2) standardized effect sizes. The raw effect sizes are summary of the results that are expressed in the same units as the raw data. For example, when kilograms are measured, a raw effect size reports a measure in kilogram. Consider the effect of a diet on a treatment group; a control group receives no diet. The change in weight can be expressed as the mean difference between the group. This measure is also in kg and so is a raw effect size. Standardized effect sizes are expressed on a standardized scale which has no longer any unit but which have a universal interpretation. A z score is an example of a standardized measure. This document is concerned exclusively on standardized effect sizes.

## Benchmarks

What makes an effect size "large" or "small" is completely dependent on the context of the study in question. However, it can be useful to have some loose criterion in order to guide researchers in effectively communicating effect size estimates. Jacob Cohen [-@cohen1988], the pioneer of estimation statistics, suggested many conventional benchmarks (i.e., how we refer to an effect size other than using a number) that we currently use. However, @cohen1988 noted that labels such as "small", "medium", and "large" are relative, and in referring to the size of an effect, the discipline, the context of research, as well as the research method and goals, should take precedence over benchmarks any time it's possible. There are general differences in effect sizes across different disciplines, and within each discipline, effect sizes differ depending on study designs and research methods [@schäfer2019] and goals; as @glass1981a explains:

> Depending on what benefits can be achieved at what cost, an effect size of 2.0 might be "poor" and one of .1 might be "good."

Therefore, it is crucial to recognize that benchmarks are only general guidelines, and importantly, out of context. They also are tend to attract controversy [@glass1981a; @kelley2012; @harrell2020]. Note that empirical benchmarks have been suggested by researchers. For social psychology, these alternative benchmarks obtained through meta-analyzing the literature (for example, [this](https://doi.org/10.1037/1089-2680.7.4.331) and [this](https://doi.org/10.1016/j.paid.2016.06.069); see [this Twitter thread](https://twitter.com/cjsotomatic/status/1144701540839698432) for a summary) are typically smaller than what Cohen put forward. Please refer to the table below:

+-------------------------------+----------------------+-----------+-----------+-----------+
| Effect Size                   | Reference            | Small     | Medium    | Large     |
+===============================+======================+:=========:+:=========:+:=========:+
| *Mean Differences*            |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's $d$ or Hedges' $g$    | @cohen1988[^2]       | 0.20      | 0.50      | 0.80      |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               |                      | 0.18      | 0.37      | 0.60      |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               | @lovakov2021[^3]     | 0.15      | 0.36      | 0.65      |
+-------------------------------+----------------------+-----------+-----------+-----------+
| *Correlational*               |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Correlation Coefficient ($r$) | @cohen1988           | .10       | .30       | .50       |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               | @richard2003[^4][^5] | .10       | .20       | .30       |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               | @lovakov2021         | .12       | .24       | .41       |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               | @paterson2016        | .12       | .20       | .31       |
+-------------------------------+----------------------+-----------+-----------+-----------+
|                               | @bosco2015           | .09       | .18       | .26       |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's $f^2$                 |                      | .02       | .25       | .40       |
+-------------------------------+----------------------+-----------+-----------+-----------+
| eta-squared ($\eta^2$)        | @cohen1988           | .01       | .06       | .14       |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's $q$                   |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's f                     | @cohen1988           | .10       | .25       | .40       |
+-------------------------------+----------------------+-----------+-----------+-----------+
| *Categorical*                 |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's omega                 | @cohen1988           | 0.10      | 0.30      | 0.50      |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Phi                           | @cohen1988           | .10       | .30       | .50       |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cramer's V                    |                      | [^6]      |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Odds ratio                    |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Relative risk                 |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Risk difference               |                      |           |           |           |
+-------------------------------+----------------------+-----------+-----------+-----------+
| Cohen's $h$                   | @cohen1988           | 0.2       | 0.5       | 0.8       |
+-------------------------------+----------------------+-----------+-----------+-----------+

[^2]: @sawilowsky2009 expanded Cohen's benchmarks to include also very small effects ($d$ = 0.01), very large effects ($d$ = 1.20), and huge effects ($d$ = 2.0). It has to be noted that very large and huge effects are very rare in experimental social psychology.

[^3]: According to this recent meta-analysis on the effect sizes in social psychology studies, "It is recommended that correlation coefficients of .1, .25, and .40 and Hedges' $g$ (or Cohen's $d$) of 0.15, 0.40, and 0.70 should be interpreted as small, medium, and large effects for studies in social psychology.

[^4]: "The current review proposes an empirical basis for gauging the size of social psychological effects. It indicates that a correlation coefficient of .10 is 'small' relative to most social psychological effects. Mean effects this small are found in roughly 30% of social psychological research literature. It indicates that a correlation coefficient of .20 is a medium-sized effect. Effects that small are found in roughly half of the relevant literature. A correlation coefficient of .30 is large relative to most social psychological effects. Less than 25% of mean effects are that large."

[^5]: These benchmarks are also recommended by @gignac2016. @funder2019 expanded them to also include very small effects ($r$ = .05) and very large effects ($r$ = .40 or greater). According to them, \[...\] an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size $r$ of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size $r$ of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication." But see [here](https://twitter.com/aaronjfisher/status/1168252264600883200?s=20) for controversies with this paper.

[^6]: The benchmarks for Cramer's V are dependent on the size of the contingency table on which the effect is calculated. According to Cohen, use benchmarks for phi coefficient divided by the square root of the smaller dimension minus 1. For example, a medium effect for a Cramer's V from a 4 by 3 table would be .3 / sqrt(3 - 1) = .21.

It should be noted that small/medium/large effects do not necessarily mean that they have small/medium/large practical implications [for details see, @coe2012; @pogrow2019]. These benchmarks are more relevant for guiding our expectations. Whether they have practical importance depends on contexts. To assess practical importance, it will always be desirable for standardized effect sizes to be translated to increase/decrease in raw units (or any meaningful units) or a Binomial Effect Size Display (roughly, differences in proportions such as success rate before and after intervention).

**Please also note that only zero means no effect**. An effect of the size .01 is an effect, but a very small [@sawilowsky2009], and likely unimportant, one. It makes sense to say that "we failed to find evidence for rejecting the null hypothesis," or "we found evidence for only a small/little/weak-to-no effect" or "we did not find a meaningful effect". **It does not make sense to say, "we found no effect."** Purely by the random nature of our universe, it is hard to imagine that we can obtain a sharp zero-effect result. This is also related to the crud factor, which refers to the idea that "everything correlates with everything else" [@orben2020, pp. 1; @meehl1984], but the practical implication of very weak/small correlations between some variables may be limited, and whether the effect is reliably detected depends on statistical power.

## Reporting Effect Sizes

### Transparency

When reporting effect sizes and their calculations, you should prioritize transparency and reproducibility. No matter what tool you used to calculate your effect size (R is the most recommended tool here), you must make sure that others can easily follow your procedures and obtain the same results. This means that if you use online calculators (which is discouraged) or standalone programs (JAMOVI is most recommended; you can also use JASP, which however does not allow access to syntax at this moment), you should include screenshots that capture the input and output, with clear explanations. If you use R, Python or other programming languages, you should copy-and-paste your codes into your supplementary document (or submit your scripts to open online repositories), ideally with annotations and comments explaining the codes. inputs and outputs.

### Directionality

Some effect sizes are directional (e.g., Cohen's $d$), which means that they can be positive or negative. Their signs carry important information, and therefore cannot be omitted. When you report these effect sizes, make it clear what is compared to what (i.e., the direction of comparison). Better still, make sure your comparison is inline with the theory. For instance, a theory predicts that your group X should score higher on an item than your Group Y,[^7] you should hypothesize accordingly that Group X will have a higher mean than Group Y on the item, and subtract mean(Y) from mean(X) (rather than the other way around) to obtain the mean difference. You should then expect your $t$ statistic to be positive, and your $d$ value as well. In other words, avoid reporting anything like $t$ = -5.14, $d$ = 0.36, where the signs of the statistics do not match.

[^7]: Of course, if a theory/effect predicts Group X has a higher mean than Group Y, then it also predicts the reverse, i.e., Group Y has a lower mean than Group X. But theories/effects are commonly articulated in a certain way. It is more common that we say, for example, people prefer the status quo rather than that people do not prefer the non-status quo, when we refer to the status quo bias. Consider another "theory": teenagers get taller when they get older. It just does not make sense to say the same thing reversely, i.e., teenagers get shorter when they get younger, because people cannot get younger, at least in the 2020s.

### Precision

Effect sizes may be very precisely estimated from the available data, the used methodology, and how the population was sampled. It might also be estimated with little confidence on the resulting number. This may be the case for example when the sample is very small, when the population displays a lot of variability, when a between-group design is used instead of a paired-sample design, and finally, when clustered sampling is used instead of randomized sampling. Precision can be estimated using various tools, but probably the most commonly used one is the Confidence intervals. This interval has a confidence level, frequently 95%.

## Interpreting Confidence Intervals

What is the correct interpretation of a confidence interval? Imagine you conducted a study where you compared two groups. You obtained a Cohen's $d$ = 0.3, 95% CI \[0.2, 0.4\]. How do you interpret this confidence interval?

Confidence intervals are yielded by a certain procedure, such that when the procedure is repeatedly applied to a series of hypothetical datasets drawn from the studied population/populations, it yields intervals that contain the true parameter value (in our example, it means the true difference between the two groups) in 95% of the cases.

In colloquial terms, if we conduct this research over and over (repeating the same sampling procedure, administering the same experimental manipulation, conducting the same statistical analysis, etc.), because of sampling variability (our samples are slightly different at each time), we will get different Cohen's $d$ values. For each of these $d$ values, we calculate a 95% interval. Then, among all these many intervals, we expect that 95% of them will contain the true $d$, which we never know exactly.

There is also a common criticism levied against the confidence interval interpretation: "There is a 95% probability that the true parameter exists within the 95% confidence interval". However this criticism is unwarranted in the specific case of a single observed confidence interval, that is, as long as there is a single realized confidence interval sampled from the population, this interpretation is fine [@vos2022]. It is important to note however, this interpretation is incorrect when there are multiple realized confidence intervals. The criticized interpretation also tends to be more practical than the interpretation using repeated sampling, the following example described by @vos2022 illustrates this,

> The distinction between these interpretations can be understood with the simple example of the probability of rolling a '6' with a fair die. The probability is 1/6 because if you roll the die repeatedly the proportion of times that the face with '6' comes up will be come very close to 1/6. Or, the probability is 1/6 because it is equivalent to a random selection from an urn where exactly one of 6 balls is labelled with '6'. The distinction in this simple example is less useful since repeatedly rolling a die is less problematic than repeatedly conducting the same randomized trial.

For further reading on confidence interpretations, see @hoekstra2014 and @morey2016.

## Reporting Confidence Intervals

Confidence intervals must be calculated and reported for every effect size that you obtained and mentioned in your manuscript. If you are doing a replication and your target article/study did not report CIs for its effect sizes, you should calculate CIs and report them.

Normally, we calculate 95% confidence intervals (i.e., 95% of such intervals are expected to contain the true parameter value if we conduct an infinite number of identical studies). Nonetheless, for some effect sizes (e.g., eta-squared, partial eta-squared, R-squared), we calculate 90% confidence intervals. This is because η² is squared and always positive, and F-tests are one-sided. Reporting 95% CI for eta squared may result in situations in which the CI includes zero but the p-value falls below .05, whereas reporting 90% CI prevents such a problem. For further information regarding this issue, read Daniel Lakens blog on confidence intervals and @steiger2004.

Confidence intervals should be reported immediately after an effect size, e.g., Cohen's d = 0.40, 95% CI \[0.20, 0.60\]. After the first time reporting them in a manuscript, every subsequent CI can be simply denoted by brackets without the "95% CI" preceding it.

Unless you are measuring something that is meaningful in real life (e.g., income, years of experience, amount that a person is willing to donate), please make sure that the CI you calculated is a CI of the effect size, not of other statistics, such as the test statistics or mean difference in raw units.

If you see one of the following:

1.  Your effect size estimate does not fall in your confidence interval: you certainly have an issue.

2.  One of your CI bound is "infinite"

3.  Your effect size estimate is not included within your CI (for comparison between two groups): You have an issue, check carefully. For means and for difference in means, the estimate should be precisely the midpoint of your CI; for other statistics (e.g., correlation, proportion, frequency, standard deviation), one arm might be longer than the other so the estimate may not be the midpoint.

For further reading related to the calculaton and reporting of effect sizes and confidence intervals, see @steiger2004 and @lakens2014.

## Useful R Packages

The following R packages are handy for effect size and CI calculations, conversions among different effect sizes, and conversion of test statistics to effect sizes. If you use one of the packages below, please make sure you cite them to give the authors their due credit! To obtain citations for packages, you can use the `citation()` function and input the name of the package as a string.

-   `MOTE`: This is a highly recommended package for calculating effect sizes, which is capable of handling a wide variety of effect sizes in the difference family (the d family) and variance-overlap family (r, eta, omega, epsilon). The functions also provide non-central confidence intervals for each effect size and output in APA style in LaTeX. `MOTE` has an online shiny application ([doomlab.shinyapps.io/mote/](https://doomlab.shinyapps.io/mote/)). The CRAN project can be found here: [cran.r-project.org/package=MOTE](https://cran.r-project.org/package=MOTE).

-   `effectsize`: This package is particularly useful in data analysis. A major advantage of this package is that it takes in many different model objects and directly outputs effect sizes and CIs. It also does some conversion. The CRAN project can be found here: [cran.r-project.org/package=effectsize](https://cran.r-project.org/package=effectsize).

-   `MBESS`: One of the most comprehensive and useful packages for effect size and confidence interval calculations. It provides functions that can calculate ESs and CIs from test statistics and the p-value. The CRAN project can be found here: [cran.r-project.org/package=MBESS](https://cran.r-project.org/package=MBESS).

-   `metafor`: Probably the most comprehensive meta-analysis package currently available. Includes the function, `escalc()`, that calculates various types of effect sizes from test-statistics, summary statistics, and more. The CRAN project can be found here: [cran.r-project.org/package=metafor](https://cran.r-project.org/package=metafor).

-   `psych`: One of the most comprehensive and general packages for common statistical procedures in psychology research. It also includes some effect size and CI calculation functions (e.g., `cohen.d()`). The CRAN project can be found here: [cran.r-project.org/package=psych](https://cran.r-project.org/package=psych).

-   `esc` (recommended for `d`, phi, and conversion among effect sizes): This package can help convert among different effect sizes (pp. 4-12 in the reference manual). It's also helpful when only incomplete information (e.g., only descriptives, or only p-values) have been provided in the paper, and we want to calculate effect sizes from them. Another package that provides similar conversion functions is the `compute.es` package. The CRAN project can be found here: [cran.r-project.org/package=esc](https://cran.r-project.org/package=esc).

-   `psychmeta`: This package is mainly used for psychometric meta-analyses. It has a function for converting different effect sizes/test statistics (convert_es, p. 38 in the reference manual), including $r$, $d$, $t$-statistic (and its p-value), $F$ (and its p-value in two-group one-way ANOVA), chi-squared (one degree of freedom), etc., to $r$, $d$ and the common language effect sizes (CLES, A, AUC). The CRAN project can be found here [cran.r-project.org/package=psychmeta](https://cran.r-project.org/package=psychmeta).

-   `effsize`: This is a relatively lightweight package that handles d, g, Cliff delta, and Vargha-Delaney A). The CRAN project can be found here: [cran.r-project.org/package=effsize](https://cran.r-project.org/package=effsize).

-   `MAd`: This package is a collection of functions for conducting a meta-analysis with mean differences data. It also provides conversion functions. The CRAN project can be found here: [cran.r-project.org/package=MAd](https://cran.r-project.org/package=MAd).

-   `DeclareDesign`: This simulation framework can be used to assess whether procedures for calculating confidence intervals are valid and can be used for arbitrary designs. The `diagnose_design()` function calculates coverage for designs with estimation strategies that produce confidence intervals. The CRAN project can be found here: [cran.r-project.org/package=DeclareDesign](https://cran.r-project.org/package=DeclareDesign).

## T-tests

T-tests are the most commonly used statistical tests for examining differences between group means, or examining a group mean against a constant. Calculating effect sizes for t-tests is fairly straightforward. Nonetheless, there are cases where crucial figures for the calculation are missing (which happens quite often in older articles), and therefore we document methods that make use of partial information (e.g., only the M and the SD, or only the t-statistic and df) for the calculation.

### Commonly used effect sizes for t-tests

Before we proceed, we need to know the commonly used effect sizes for t-tests, including Cohen's $d$, Hedges' $g$, and Glass' $\Delta$. There are others, such as the root-mean-square standardized effect ($\Psi$), but they are rarely reported or used for power analysis.

| Name        | Description    |
|:------------:|:-------------------------------|
| Cohen's $d$ | Cohen's d is the most commonly used and reported effect size for comparison between two groups (or one group against a constant in the one-sample t-test). It is the difference between the means of two groups divided by standard deviation assuming both groups have the same population standard deviation ($\sigma$). |
| Hedges' $g$ | Hedges' $g$ is preferred over Cohen's $d$ when sample size ($n$) is small (conventionally, group size \< 20). When sample size is not small (group size \> 50), the two do not differ much, however Hedges' $g$ is an unbiased estimate of the population standardized mean difference at every sample size and therefore should be used in all cases. Hedges' $g$ is simply Cohen's $d$ with a multiplicative correction factor (i.e., a small sample correction factor) appended to it. Cohen's d can be converted to Hedges' g very easily with the hedges_g function in the esc package in R. | 
| Glass' $\Delta$ | Glass’ $\Delta$ differs from Cohen’s $d$ and Hedges’ $g$ in that it uses only the standard deviation of one group under comparison (typically the control group, i.e., the group that is compared with, rather than the treatment group). This effect size measure is used when the two groups’ standard deviations differ substantially (i.e., the homogeneity assumption is not met).|

In fact, the use of the Cohen's $d$ and Hedges' $g$ notations are very inconsistent in both statistical methods papers and empirical research papers (see here). Therefore, reproducibility is important - make people see your calculations so that they will know not only what effect sizes you calculated, but also how you did so.

## Calculating Different *d* Values

There are multiple types of Cohen's $d$s, yet researchers very often do not identify which $d$ they are reporting [see @lakens2013]. Here we catalog the equations for each type of $d$ value. A $d$ value calculated from a sample will also contain sampling error, therefore we will also show the equations to calculate the standard error. The standard allows us to then calculate the confidence interval. For each formulation in the sections below, the confidence interval will be able to be calculated in the same way, that is,

$$
\text{Lower Bound} = d - 1.96\times SE
$$

$$
\text{Upper Bound} = d + 1.96\times SE
$$

Lastly, we will supply example R code so you can apply to your own data.

### Single Group Designs
For a single group design, we have one group and we want to compare the mean of that group to some constant, $C$ (i.e., a target value). The standardized mean difference for a single group can be calculated by,

$$
d_1 = \frac{M_1-C}{S_1}
$$  
A positive $d_1$ value would indicate that the mean of group 1 is larger than the target value, $C$. This formulation assumes that the sample is drawn from a normal distribution. The standardizer (i.e., the denominator) is the sample standard deviation. The corresponding standard error for $d_1$ is,

$$
SE_{d_1} = \sqrt{\frac{1}{n}+\frac{d_1^2}{2n}}.
$$

In R, we can use the `d.single.t` function from the `MOTE` package to calculate the single group standardized mean difference. 

```{r,echo=TRUE}
# Install packages if not already installed:
# install.packages('MOTE')
# Cohen's d for one group

# For example:
# Sample Mean = 30.4, SD = 22.53, N = 96
# Target Value, C = 15

library(MOTE)

stats <- d.single.t(
  m = 30.4,
  u = 15,
  sd = 22.53,
  n = 96
)

# print just the d value and confidence intervals
rbind(stats$d,
      stats$dlow,
      stats$dhigh)

```
As you can see, the output shows that the effect size is $d_b$ = 0.68, 95% CI [0.46, 0.90].

### Two Groups Design
For a two group design (i.e., between-groups design), we want to compare the means of two groups (group 1 and group 2). The standardized mean difference between two groups can be calculated by,

$$
d_p = \frac{M_1-M_2}{S_p}.
$$  

A positive $d_p$ value would indicate that the mean of group 1 is larger than the mean of group 2. Dividing the mean difference by the pooled standard deviation, $S_p$, is the classic formulation of Cohen's $d$. The pooled standard deviation, $S_p$, can be calculated as the square root of the average variance (weighted by the degrees of freedom, $df=n-1$) of group 1 and group 2:

$$
S_p = \sqrt{\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}}
$$

Note that the term *variance* refers to the square of the standard deviation ($S^2$). The corresponding standard error of $d_p$ is,

$$
SE_{d_p} = \sqrt{\frac{n_1+n_2}{n_1 n_2}+\frac{d_p^2}{2(n_1+n_2)}}.
$$

In R, we can use the `d.ind.t` function from the `MOTE` package to calculate the two group standardized mean difference. Since we have already loaded in the `MOTE` package, we do not need to again. 

```{r,echo=TRUE}
# Install packages if not already installed:
# install.packages('MOTE')
# Cohen's d for two independent groups
# given means and SDs

# For example:
# Group 1 Mean = 30.4, SD = 22.53, N = 96
# Group 2 Mean = 21.4, SD = 19.59, N = 96

stats <- d.ind.t(
  m1 = 30.4,
  m2 = 21.4,
  sd1 = 22.53,
  sd2 = 19.59,
  n1 = 96,
  n2 = 96,
  a = 0.05
)

# print just the d value and confidence intervals
rbind(stats$d,
      stats$dlow,
      stats$dhigh)

```

The output shows that the effect size is $d_b$ = 0.43, 95% CI [0.14, 0.71].


### Repeated Measures Designs

In a repeated measures design, the same subjects are measured on two separate occasions and we want to know the mean difference between those two occasions. An example of this would be in a pre/post comparison where subjects are tested before and after undergoing some treatment (see @fig-repeatedmeasures for a visualization). A standardized mean difference in a repeated measures design can take on a few different forms that we define below.

```{r,message=FALSE,echo=FALSE,warning=FALSE}
#| id: fig-repeatedmeasures
#| fig-cap: Figure displaying simulated data of a repeated measures design, the x-axis shows the condition (e.g., pre-test and post-test) and y-axis is the scores. Lines connect the change within subject from one condition to the next. 

library(ggdist)
library(MASS)
set.seed(343)
r = .7
n = 50
data<- mvrnorm(n = n,
               mu = c(21.4,30.4),
               Sigma = data.frame(x = c(22.53^2,r*22.53*19.59),
                                  y = c(r*22.53*19.59,19.59^2)),
               empirical=TRUE)

df <- data.frame(X = c(data[,1],data[,2]),
                 ID = c(1:n,1:n),
                 Condition = c(rep(1,n),rep(2,n)))

ggplot(data=df,aes(x=Condition,y=X,group=ID)) +
  geom_jitter(color='grey30',width=.015,height=0,alpha=.8,size=2) +
  geom_line(color='grey30',alpha=.3) +
  theme_ggdist() +
  theme(aspect.ratio = 1,
        axis.text = element_text(size=13),
        axis.title = element_text(size=14),
        title = element_text(size=14)) +
  stat_slab(data=df[df$Condition==1,],aes(group=Condition,x=Condition-.15,y=X), 
            position = position_dodge(),scale=.5,alpha=.3,side='left') +
  stat_slab(data=df[df$Condition==2,],aes(group=Condition,x=Condition+.15,y=X), 
            position = position_dodge(),scale=.5,alpha=.3,side='right') +
  stat_pointinterval(data=df[df$Condition==1,],aes(group=Condition,x=Condition-.15,y=X),color = 'grey30') +
  stat_pointinterval(data=df[df$Condition==2,],aes(group=Condition,x=Condition+.15,y=X),color = 'grey30') +
  ggtitle("Repeated Measures Design") +
  scale_x_continuous(breaks=c(1,2))
  

```

#### Difference Score $d$ ($d_z$)
Instead of comparing the means of two sets of scores, a within subject design allows us to subtract the scores obtained in condition 1 from the scores in condition 2. These difference scores ($X_{\text{diff}}=X_2-X_1$) can be used similarly to the single group design (if the target value was zero, i.e., $C=0$) such that,

$$
d_z = \frac{M_{\text{diff}}}{S_{\text{diff}}}
$$

Where the difference between this formulation and the single group design is the nature of the scores (difference scores rather than raw scores). The convenient thing about $d_z$ is that it has a straight-forward relationship with the $t$-statistic, $d_z=\frac{t}{\sqrt{n}}$. This makes it very useful for power analyses. If the standard deviation of difference scores are not accessible, then it can be calculated using the standard deviation of condition 1 ($S_1$), the standard deviation of condition 2 ($S_2$), and the correlation between conditions ($r$):

$$
S_{\text{diff}}=\sqrt{S^2_1 + S^2_2 - 2 r S_1 S_2}
$$

It is important to note that when the correlation between groups is large, then the $d_z$ value will also be larger, whereas a small correlation will return a smaller $d_z$ value. The standard error of $d_z$ can be calculated similarly to the single group design such that,

$$
SE_{d_z} = \sqrt{\frac{1}{n}+\frac{d_z^2}{2n}}
$$

In R, we can use the `d.ind.t.diff` function from the `MOTE` package to calculate $d_z$.

```{r,echo=TRUE}
# Install packages if not already installed:
# install.packages('MOTE')
# Cohen's dz for difference scores

# For example:
# Difference Score Mean = 21.4, SD = 19.59, N = 96

library(MOTE)

stats <- d.dep.t.diff(
  m = 21.4,
  sd = 19.59,
  n = 96,
  a = 0.05
)

# print just the d value and confidence intervals
rbind(stats$d,
      stats$dlow,
      stats$dhigh)

```
The output shows that the effect size is $d_b$ = 1.09, 95% CI [0.84, 1.34].


#### Cohen's Repeated Measures $d$ ($d_{rm}$)

For a within-group design, we want to compare the means of scores obtained from condition 1 and condition 2. The repeated measures standardized mean difference between the two conditions can be calculated by,

$$
d_{rm} = \frac{M_2-M_1}{S_w}.
$$  

A positive $d_{rm}$ value would indicate that the mean of condition 2 is larger than the mean of condition 1. The standardizer here is the within-subject standard deviation, $S_w$. The within-subject standard deviation can be defined as,

$$
S_{\text{diff}}=\sqrt{\frac{S^2_1 + S^2_2 - 2 r S_1 S_2}{2(1-r)}}.
$$

We can also express $S_w$ in terms of $S_{\text{diff}}$,

$$
S_w = \frac{S_{\text{diff}}}{ \sqrt{2(1-r)} }.
$$

Furthermore, we can even express $d_{rm}$ in terms of $d_z$,

$$
d_{rm} = d_z \times \sqrt{2(1-r)} .
$$

Ultimately the $d_{rm}$ is more appropriate as an effect size estimate for use in meta-analysis whereas $d_z$ is more appropriate for power analysis [@lakens2013]. The standard error for $d_{rm}$ can be computed as,

$$
SE_{d_{rm}} = \sqrt{\left(\frac{1}{n} + \frac{d^2_{rm}}{2n}\right) \times 2(1-r)}
$$

In R, we can use the `d.ind.t.rm` function from the `MOTE` package to calculate the repeated measures standardized mean difference ($d_{rm}$).

```{r,echo=TRUE}
# Cohen's d for two independent groups
# given means and SDs

# For example:
# Condition 1 Mean = 30.4, SD = 22.53, N = 96
# Condition 2 Mean = 21.4, SD = 19.59, N = 96
# correlation between conditions: r = .40

stats <- d.dep.t.rm(
  m1 = 30.4,
  m2 = 21.4,
  sd1 = 22.53,
  sd2 = 19.59,
  r = .40,
  n = 96,
  a = 0.05
)

# print just the d value and confidence intervals
rbind(stats$d,
      stats$dlow,
      stats$dhigh)

```

The output shows that the effect size is $d_b$ = 0.42, 95% CI [0.21, 0.63].



#### Average Variance $d$ ($d_{av}$)



#### Becker's $d$ ($d_b$)




